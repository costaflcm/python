# -*- coding: utf-8 -*-
"""spark_df_vendas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VPhOrhJF5Pva-f7zTa8vr-sJjJ67Hz0m
"""

# ==============================================================================
# Sessão Pré Reqs
# ==============================================================================
# Script   | spark_df_vendas.py
# ---------|--------------------------------------------------------------------
# Objetivo | Preparar os dados de Vendas para consumo em ferramentas de
#          | Data Visualization, como PowerBI ou Tableau
# ---------|--------------------------------------------------------------------
# Pré-Req  | Python 3.0, e Spark 3.4.1 ou superior
# ---------|--------------------------------------------------------------------
# Nota     | Os dados podem ser amazanados em outros diretórios, não senndo
#          | necessário o uso do google drive
# ==============================================================================

# Verifica a Vesão do Python
!python --version

# Instala o ambiente do Spark no Google Colab
!pip install pyspark
!spark-submit --version

# ==============================================================================
# Sessão Configurações Gerais
# ==============================================================================

# Pacotes Python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from google.colab import drive

# Conexão com Google Drive
drive.mount('/content/drive')

# Conexão com o Spark
spark = SparkSession.builder\
  .master('local[*]')\
  .appName('Public Data Transform')\
  .getOrCreate()

# Variáveis e Atributos
filepathin = '/content/drive/MyDrive/Python/data/in/vendas/'
filepathout = '/content/drive/MyDrive/Python/data/out/vendas/'

# ==============================================================================
# Sessão de Tratamento dos Dados de Clientes
# ==============================================================================

# RJ
dados = spark.read.csv(filepathin+'CLIENTES_RJ.csv', header=True, inferSchema=True)

# Preparação dos dados
clientes_rj = dados.select(
  lpad(col('CPF'),11,'0').alias('nr_cpf'),
  col('NOME').alias('nm_cliente'),
  col('BAIRRO').alias('nm_bairro'),
  lit('Rio de Janeiro').alias('nm_cidade'),
  col('ESTADO').alias('nm_estado'),
  col('GENERO').alias('tp_genero')
)

clientes_rj.printSchema()
clientes_rj.show(n=50, truncate=False)

# SP
dados = spark.read.csv(filepathin+'CLIENTES_SP.csv', header=True, inferSchema=True)

# Preparação dos Dados
clientes_sp = dados.select(
  lpad(col('CPF'),11,'0').alias('nr_cpf'),
  col('NOME').alias('nm_cliente'),
  col('BAIRRO').alias('nm_bairro'),
  col('CIDADE').alias('nm_cidade'),
  col('ESTADO').alias('nm_estado'),
  col('SEXO').alias('id_genero')
)

clientes_sp.printSchema()
clientes_sp.show(n=50, truncate=False)

# União dos Dataframes e Export para o Formato Parquet
clientes_df = clientes_rj.unionAll(clientes_sp)
clientes_df.write.parquet(filepathout+'clientes.parquet',mode='overwrite')

# ==============================================================================
# Sessão de Tratamento dos Dados de Vendedores
# ==============================================================================
dados = spark.read.csv(filepathin+'VENDEDORES.csv',header=True, inferSchema=True)

# Renomear as colunas
vendedores = dados\
  .withColumnRenamed('MATRICULA','cd_matricula')\
  .withColumnRenamed('NOME','nm_vendedor')

vendedores.printSchema()
vendedores.show()

# Export para parquet
vendedores.write.parquet(filepathout+'vendedores.parquet',mode='overwrite')

# ==============================================================================
# Sessão de Tratamento dos Dados de Produtos
# ==============================================================================
dados = spark.read.csv(filepathin+'PRODUTOS.csv', header=True, inferSchema=True)

# Tratamento dos dados
produtos = dados.select(
    lpad('CODIGO_PRODUTO',10,'0').alias('cd_produto'),
    split('NOME_PRODUTO','-')[0].alias('nm_produto'),
    split(split('NOME_PRODUTO','-')[1],' ')[1].alias('nr_peso'),
    split(split('NOME_PRODUTO','-')[1],' ')[2].alias('tp_medida'),
    split('NOME_PRODUTO','-')[2].alias('nm_marca')
).withColumn(
  'tp_medida',
  when(
      col('tp_medida').isin('Litros', 'Litro'),
      'L'
      ).otherwise('ML'))

produtos.printSchema()
produtos.show(n=5, truncate=False)

# Export dos Dados
produtos.write.parquet(filepathout+'produtos.parquet',mode='overwrite')

# ==============================================================================
# Sessão de Tratamento dos Dados de Itens
# ==============================================================================
dados = spark.read.csv(filepathin+'ITENS.csv', header=True, inferSchema=True)
dados.printSchema()
dados.show(n=5, truncate=False)

# ==============================================================================
# Sessão de Tratamento dos Dados Notas Fiscais
# ==============================================================================

# Juntando os dados dos Itens com as Notas
dados_itens = spark.read.csv(filepathin+'ITENS.csv', header=True, inferSchema=True)
dados_notas = spark.read.csv(filepathin+'NOTAS.csv', header=True, inferSchema=True)
dados = dados_itens.join(dados_notas, 'NUMERO_NOTA', how='left')

# Prepara os dados para agregação
dados = dados.select(
  date_format('DATA_VENDA', 'yyyy-MM').alias('dt_venda'),
  lpad(col('CODIGO_CLIENTE'),11,'0').alias('nr_cpf'),
  col('MATRICULA').alias('cd_matricula'),
  lpad('CODIGO_PRODUTO',10,'0').alias('cd_produto'),
  col('QUANTIDADE').alias('qt_itens'),
  (col('PRECO')*col('QUANTIDADE')).alias('VL_FATURAMENTO'),
)

# Agrupamento dos Dados
vendas = dados\
  .groupBy('dt_venda', 'nr_cpf', 'cd_matricula', 'cd_produto')\
  .sum('qt_itens','vl_faturamento')\
  .withColumnRenamed('sum(qt_itens)', 'qt_itens')\
  .withColumnRenamed('sum(vl_faturamento)', 'vl_faturamento')

vendas.printSchema()
vendas.show(n=5, truncate=False)

# Export dos Dados
vendas.write.parquet(filepathout+'vendas.parquet',mode='overwrite')

# Encerra a conexão com o Spark
spark.stop()